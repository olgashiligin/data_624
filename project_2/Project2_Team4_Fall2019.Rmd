---
title: "Data 624 - Final Project (Team 4)"
author: "Team 4"
date: "30/11/2019"
output: html_document
---

##Introduction

The purpose of this project is to predict level of PH in beverages. 

pH plays an important role in food processing and beverages in particular. This project will explain the factors used in the production of beverages and how these factors have an impact on the pH level of these beverages. 
More specifically, we are examining the underlying factors of the pH level of beverages that could potentially be a powerful predictor of the pH level and finally make a predictions of the pH values of beverages based on these factors.

StudentEvaluation.xlsx data set will be used to train and test models. 

StudentData.xlsx data set will be used to make PH predictions.


###Data Exploration

```{r message = F}
# loading necessary packages
library(plyr)
library(tidyverse)
library (e1071)
library(corrplot)
library(ggplot2)
library(tidyr)
library(dplyr)
library(caret)
```

Summary of the data set for training and testing models:

```{r message = F}
#  reading file for training and testing models
ph_data<- read.csv("https://raw.githubusercontent.com/olgashiligin/data_624/master/project_2/StudentData.csv")
#  checking data structure of the file
str(ph_data)
```

The training data set contains 2571 observations and 33 variables. Brand.Code is a character variable, the remaining variables are numeric. PH is the response variable.

Summary of the data set for making predictions:

```{r message = F}
#  reading file for making predictions
pred_data <- read.csv("https://raw.githubusercontent.com/olgashiligin/data_624/master/project_2/StudentEvaluation.csv")
#  checking data structure of the file
str(pred_data)
```

The data set contains 267 observations and 33 variables. Brand.Code is a character variable, the remaining variables are numeric. PH is the variable that should be predicted.

### Target Variable Distribution (pH)

Target variables distribution (overall). 

```{r message = F}
# removing rows with missing target variable values (4 rows removed)
ph_data<-ph_data %>% drop_na(PH)
plot(density(ph_data$PH),main = 'pH Value Distribution', xlab = 'pH', ylab = "")
```

Target variables distribution (by Brand Code).

```{r message = F}
ggplot(ph_data, aes(x=PH)) + 
  geom_density()+
    facet_wrap(~ Brand.Code, scales = "free_y",ncol = 1)+
    geom_vline(aes(xintercept=mean(PH)),
            color="blue", linetype="dashed", size=1)+
    labs(title = "pH Value Distribution by Brand Code")
```


pH is close to normally distributed variable with the range from 8 to 9. Variations of the target variable distribution by Brand Code have been presented on the graph above. 

### Summary Statistics

Checking summary statistics of the numeric predictors.

```{r message = F}
means <- sapply(ph_data[-1], function(y) mean(y, na.rm = TRUE))
medians <- sapply(ph_data[-1], function(y) median(y, na.rm = TRUE))
vars <- sapply(ph_data[-1], function(y) var(y, na.rm = TRUE))
skews <- sapply(ph_data[-1], function(y) skewness(as.numeric(y), na.rm = TRUE))
IQRs <- sapply(ph_data[-1], function(y) IQR(y, na.rm = TRUE))
NAs <- sapply(ph_data[-1], function(y) sum(length(which(is.na(y)))))

summary <- data.frame(means, medians, IQRs, vars, skews, NAs)
colnames(summary) <- c("MEAN", "MEDIAN", "Var", "SKEW","IQR","NAs")
summary <- round(summary, 2)

summary
```

### Visualization of the numeric predictors

Visualizing distribution of the numeric predictors.

```{r message = F}
ph_data %>%
  select(-Brand.Code) %>%
  gather(Variable, Values) %>%
  ggplot(aes(x = Values)) +
  geom_histogram(alpha = 0.2, col = "black", bins = 15) +
  facet_wrap(~ Variable, scales = "free", nrow = 6)

```

###Checking Outliers with Boxplot 

Checking outliers of the numeric predictors with boxplots

```{r message = F}
ph_data %>% 
  select(-Brand.Code) %>% 
  gather(Variable, Values) %>% 
  ggplot(aes( y = Values)) +
  geom_boxplot() +
  facet_wrap(~ Variable, scales = "free", nrow = 6)
```

Many variables are highly skewed (MFR, Filler.Speed,Oxygen.Filler,Air.Pressurer). Some of the variables have close to normal distributions, for example: Fill.Ounces, Carb.Temp etc. or follow log-normal distribution, for example: PSC.Distribution and PSC.C02.Distribution. 

The majority of variables are continious, but some of the predictors appear to be discrete: Pressure.Setpoint, Alch.Rel.

Hyd.Pressure1 to 3 have similar patterns with large number of 0 values. In case of collinearity between them and depending on their relationships to a target variable, they may be candidates for a removal.

As we already detected many variables are highly skewed with large variance and we expect that many variables have outliers. Box Plots have confirmed that assumption.

As our main focus will be on tree models, we think that centring and scaling are not necessary.

### Relationships Between the Target Variable and Predictors

Checking Type of Relationships between the target variable and predictors.

```{r message = F}
ph_data %>% 
  gather(-PH, -Brand.Code, key="Var", value="Value") %>% 
  ggplot(aes(x=PH, y=Value)) +
  geom_point(alpha=0.01, col = "black") +
  facet_wrap(~ Var, scales = "free", nrow=4)

```

The relationships between predictors and PH appear non-linear.

###Correlation

Checking correlation between predictors in order to detect collinearity.

```{r message = F}
ph_data_cor <- cor(ph_data %>% select(-Brand.Code), use="complete.obs")
corrplot(as.matrix(ph_data_cor), method = "circle")
```

Top 10 variables correlated positively with pH

```{r message = F}
top_ph_data_cor <- ph_data_cor %>% as.data.frame() %>% select(PH) %>% 
  rownames_to_column() %>% 
  arrange(desc(PH))
top_ph_data_cor %>%
  top_n(11, PH)
```

Top 10 variables correlated negatively with pH

```{r message = F}
top_ph_data_cor %>%
  top_n(-10, PH) %>%
  arrange(PH)
```

Correlation plot shows that some predictors correlate between each other. We select these predictors using findCorrelation() and set up 0.6 for the pair-wise absolute correlation cutoff.

```{r message = F}
findCorrelation(ph_data_cor, .6, names = TRUE)
```

###Near-Zero Variance Predictors

Checking near-zero variance predictors using nearZeroVar().

```{r message = F}
zero.var <- nearZeroVar(ph_data, names = TRUE) 
zero.var
```

Hyd.Pressure1 was detected as Near-Zero Variables. The best way to deal with this uninformative predictor is to remove it from the list of predictors.

```{r message = F}
# removing near-zero Variance Predictor and highly correlated predictors
ph_data <- ph_data %>%  select (-c("Hyd.Pressure1"))
```

###Missing Values

Exploring missing values using VIM library.

```{r message = F}
library(mice)
library(VIM)
aggr(ph_data, col=c('grey','pink'),  sortVars=T,numbers=T, cex.axis=0.5)
```

Missing values of the target variable.

PH variable contains just 4 missing values, we have removed rows with these missing values as we think this is the most optimal solution.

Missing values of the predictors variables.

The maximum number of missing values is 8% (MFR), which is not critical and imputation methods can be considered.
We have decided to use Multiple Imputation by Chained Equations methods for imputing missing values.

```{r message = F}
# applying MICE method for imputing missing values using mice() with default number of iteration equal to 5
mice_imputes =mice(ph_data, method = "rf", print = FALSE, seed = 143)
ph_data =complete(mice_imputes)
summary(ph_data)
```


### Splitting Data Set

Splitting dataset into training and test sets.

```{r, message=FALSE}
set.seed(123)
training.samples <- ph_data$PH %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- ph_data[training.samples, ]
test.data <- ph_data[-training.samples, ]
```

### Models Building

Different models were built and their performance was assessed using 10 folds cross validation on the training set. Then best models have been selected, further tuned and its accuracy assessed on the test set.

The list of tested models:

Non-Linear Models: MARS, SVM

Tree Based Modles: rpart2, treebag, rf, gbm, cubist, xgbTree

```{r, message=FALSE, results='hide'}
library(caretEnsemble)
set.seed(143)
#  setting up trainControl to 10 folds cross validation
control = trainControl("cv", number = 10, savePredictions=FALSE,  index = createFolds(train.data$PH, 10), verboseIter = FALSE)
# creating list of models to be tested
algorithmList = c( "earth", "svmRadial", "rpart2", "rf","treebag", "gbm", "cubist", "xgbTree")

#  training selected models on the training data set
set.seed(143)
models = caretList(PH ~ ., data=train.data, trControl=control, methodList=algorithmList) 
results = resamples(models)
summary(results)

# Box plots to compare models
scales = list(x=list(relation="free"), y=list(relation="free"))
bwplot(results, scales=scales)
```

Random Forest is the best performed models, their average RMSE is the lowest with relatively low variation among the samples.

Let's further tune Random Forest model using grid search and assess its performance on the test set.

```{r, message=FALSE, warning=FALSE}
#  tuning rf model
set.seed(143)
rf.grid <- expand.grid(.mtry = seq(1,50, by = 5))
fit.rf.tuned <- train(PH~., data=train.data, method="rf", trControl=control, tuneGrid = rf.grid, importance=T)
# selecting best tune parameters
fit.rf.tuned$bestTune
plot(fit.rf.tuned)

# assessing performance of the tuned rf model on the test set
predictions_rf_tuned <- fit.rf.tuned %>% predict(test.data)
rf_tuned_RMSE = RMSE(predictions_rf_tuned, test.data$PH)
rf_tuned_RMSE

```

Tuned random forest model showed good result on the test set: RMSE = 0.09983016. We select this model for making predictions.

Predicted vs Actual results of the tuned Random Forest model on the test set are shown on the scatter plot below.

```{r, message=FALSE}
preds_df <- data.frame(predicted = predictions_rf_tuned, actual = test.data$PH)
ggplot(preds_df) + geom_point(aes(x = predicted, y = actual)) + geom_smooth(aes(x = predicted, y = actual))
```

Using varImp() calculating variable importance for pH using RF model.

```{r, message=FALSE}
varImp(fit.rf.tuned, scale=FALSE)
plot(varImp(fit.rf.tuned, scale=FALSE))
```

###Making Predictions

Preparing dataset for making predictions

Removing predictors as we did it with training set.

```{r message = F}
pred_data <- pred_data %>% select (-c("Hyd.Pressure1"))
```

Missing Variables Imputation 

```{r message = F, warning=FALSE}
mice_imputes = mice(pred_data, method = "rf", print = FALSE)
pred_data =complete(mice_imputes)
```

Making Predictions

Making predictions of the pH level and saving it in Team4_Predictions.csv (column name of the predicted pH - "PH_pred")

```{r message = F}
# remove ph to get only predictors
pred_data <- pred_data %>% select(-PH)
# making predictions
new_resp <- predict(fit.rf.tuned, pred_data)
#write and save predictors and predicted response variable - "PH_pred" 
data.frame(pred_data, PH_pred = new_resp) %>%
  write.csv("Team4_Predictions.csv")

```




